import { LLMClient } from "../src/integrations/llm.js";
import { parseExpenseText } from "../src/core/parser.js";
import { describe, it, beforeAll, afterAll, expect, test, vi } from "vitest";
// Test data
const TEST_EXPENSES = [
  "Lunch 120k at McDonalds today",
  "Coffee 45k at Highlands Coffee yesterday",
  "Grab taxi 85k to airport this morning",
  "Grocery shopping 350k at Big C on Sunday",
  "Movie tickets 180k for two people last night",
];

describe("Anthropic LLM Integration Tests", () => {
  let llmClient: LLMClient;

  beforeAll(() => {
    const config = (global as any).testConfig.anthropic;
    console.log("ðŸ§ª Initializing Anthropic LLM client...");
    console.log("ðŸ”— Proxy URL:", config.baseURL);
    console.log("ðŸ”‘ Token:", config.apiKey.substring(0, 20) + "...");
    console.log("ðŸŽ­ Mock Mode:", config.useMock);

    // Skip real API calls in mock mode
    if (config.useMock) {
      console.log("âš ï¸  Using mock mode - no real API calls will be made");
      vi.mock("../src/integrations/llm.js", async () => {
        const actual = await vi.importActual("../src/integrations/llm.js");
        return {
          ...actual,
          LLMClient: vi.fn().mockImplementation((config) => {
            const chatMock = vi.fn();

            // Handle error testing scenarios
            if (
              config.apiKey === "invalid-key" ||
              config.apiKey === "invalid-token-key"
            ) {
              chatMock.mockRejectedValue(
                new Error("401: Unauthorized - Invalid API key"),
              );
              return {
                chat: chatMock,
                getProvider: () => "anthropic",
                getModel: () => "claude-3-5-haiku-20241022",
              };
            }

            if (config.timeout === 1) {
              chatMock.mockRejectedValue(new Error("Request timed out."));
              return {
                chat: chatMock,
                getProvider: () => "anthropic",
                getModel: () => "claude-3-5-haiku-20241022",
              };
            }

            // Mock different responses based on input
            chatMock.mockImplementation(async (messages) => {
              const lastMessage =
                messages[messages.length - 1]?.content?.toLowerCase() || "";
              const fullContext = messages
                .map((m) => m.content?.toLowerCase() || "")
                .join(" ");

              // Basic chat responses
              if (lastMessage.includes("hello")) {
                return {
                  content: "LLM test successful",
                  model: "claude-3-5-haiku-20241022",
                  usage: {
                    promptTokens: 20,
                    completionTokens: 5,
                    totalTokens: 25,
                  },
                };
              }

              // Conversation context
              if (
                fullContext.includes("42") &&
                fullContext.includes("remember")
              ) {
                return {
                  content: "The number 42",
                  model: "claude-3-5-haiku-20241022",
                  usage: {
                    promptTokens: 30,
                    completionTokens: 6,
                    totalTokens: 36,
                  },
                };
              }

              // Expense parsing responses
              if (
                lastMessage.includes("mcdonalds") ||
                lastMessage.includes("lunch 120k")
              ) {
                return {
                  content:
                    '```toon\n{\n  "action": "spend_vnd",\n  "params": {\n    "account": "Cash",\n    "vnd_amount": 120000,\n    "date": "2025-01-15",\n    "counterparty": "McDonalds"\n  }\n}\n```',
                  model: "claude-3-5-haiku-20241022",
                  usage: {
                    promptTokens: 80,
                    completionTokens: 50,
                    totalTokens: 130,
                  },
                };
              }

              if (
                lastMessage.includes("highlands") ||
                lastMessage.includes("coffee 45k")
              ) {
                return {
                  content:
                    '```toon\n{\n  "action": "spend_vnd",\n  "params": {\n    "account": "Cash",\n    "vnd_amount": 45000,\n    "date": "2025-01-14",\n    "counterparty": "Highlands Coffee"\n  }\n}\n```',
                  model: "claude-3-5-haiku-20241022",
                  usage: {
                    promptTokens: 80,
                    completionTokens: 50,
                    totalTokens: 130,
                  },
                };
              }

              // Amount parsing
              if (lastMessage.includes("850k")) {
                return {
                  content:
                    '```toon\n{\n  "action": "spend_vnd",\n  "params": {\n    "account": "Cash",\n    "vnd_amount": 850000,\n    "date": "2025-01-15",\n    "counterparty": "Taxi"\n  }\n}\n```',
                  model: "claude-3-5-haiku-20241022",
                  usage: {
                    promptTokens: 80,
                    completionTokens: 50,
                    totalTokens: 130,
                  },
                };
              }

              if (
                lastMessage.includes("50k") ||
                lastMessage.includes("50000")
              ) {
                return {
                  content:
                    '```toon\n{\n  "action": "spend_vnd",\n  "params": {\n    "account": "Cash",\n    "vnd_amount": 50000,\n    "date": "2025-01-15",\n    "counterparty": "Restaurant"\n  }\n}\n```',
                  model: "claude-3-5-haiku-20241022",
                  usage: {
                    promptTokens: 80,
                    completionTokens: 50,
                    totalTokens: 130,
                  },
                };
              }

              // Complex reasoning
              if (
                lastMessage.includes("850k") &&
                fullContext.includes("complex")
              ) {
                return {
                  content: JSON.stringify({
                    amount_vnd: 850000,
                    people_count: 4,
                    restaurant: "Pizza Company",
                    payment_method: "Techcombank card",
                    service_charge: 10,
                  }),
                  model: "claude-3-5-haiku-20241022",
                  usage: {
                    promptTokens: 120,
                    completionTokens: 60,
                    totalTokens: 180,
                  },
                };
              }

              if (
                lastMessage.includes("vietnamese") ||
                fullContext.includes("vietnamese")
              ) {
                return {
                  content: JSON.stringify({
                    amount_vnd: 50000,
                    currency: "VND",
                    description: "Mua cÃ  phÃª",
                  }),
                  model: "claude-3-5-haiku-20241022",
                  usage: {
                    promptTokens: 80,
                    completionTokens: 40,
                    totalTokens: 120,
                  },
                };
              }

              // Handle other expense cases
              if (
                lastMessage.includes("85k") ||
                lastMessage.includes("grab taxi")
              ) {
                return {
                  content:
                    '```toon\n{\n  "action": "spend_vnd",\n  "params": {\n    "account": "Cash",\n    "vnd_amount": 85000,\n    "date": "2025-01-15",\n    "counterparty": "Grab"\n  }\n}\n```',
                  model: "claude-3-5-haiku-20241022",
                  usage: {
                    promptTokens: 80,
                    completionTokens: 50,
                    totalTokens: 130,
                  },
                };
              }

              if (
                lastMessage.includes("350k") ||
                lastMessage.includes("big c")
              ) {
                return {
                  content:
                    '```toon\n{\n  "action": "spend_vnd",\n  "params": {\n    "account": "Cash",\n    "vnd_amount": 350000,\n    "date": "2025-01-13",\n    "counterparty": "Big C"\n  }\n}\n```',
                  model: "claude-3-5-haiku-20241022",
                  usage: {
                    promptTokens: 80,
                    completionTokens: 50,
                    totalTokens: 130,
                  },
                };
              }

              if (
                lastMessage.includes("180k") ||
                lastMessage.includes("movie")
              ) {
                return {
                  content:
                    '```toon\n{\n  "action": "spend_vnd",\n  "params": {\n    "account": "Cash",\n    "vnd_amount": 180000,\n    "date": "2025-01-12",\n    "counterparty": "Cinema"\n  }\n}\n```',
                  model: "claude-3-5-haiku-20241022",
                  usage: {
                    promptTokens: 80,
                    completionTokens: 50,
                    totalTokens: 130,
                  },
                };
              }

              // Account usage testing
              if (fullContext.includes("cash")) {
                return {
                  content:
                    '```toon\n{\n  "action": "spend_vnd",\n  "params": {\n    "account": "Cash",\n    "vnd_amount": 45000,\n    "date": "2025-01-15"\n  }\n}\n```',
                  model: "claude-3-5-haiku-20241022",
                  usage: {
                    promptTokens: 70,
                    completionTokens: 40,
                    totalTokens: 110,
                  },
                };
              }

              // Default response for any other input
              return {
                content: "LLM test successful",
                model: "claude-3-5-haiku-20241022",
                usage: {
                  promptTokens: 50,
                  completionTokens: 10,
                  totalTokens: 60,
                },
              };
            });

            return {
              chat: chatMock,
              getProvider: () => "anthropic",
              getModel: () => "claude-3-5-haiku-20241022",
            };
          }),
        };
      });
    }

    llmClient = new LLMClient(config);
  });

  describe("Basic LLM Chat", () => {
    it("should connect and respond to basic chat", async () => {
      const response = await llmClient.chat(
        [
          {
            role: "user",
            content:
              'Hello! Please respond with exactly "LLM test successful" and nothing else.',
          },
        ],
        { temperature: 0.1, maxTokens: 100 },
      );

      expect(response).toBeDefined();
      expect(response.content).toContain("successful");
      expect(response.content.length).toBeGreaterThan(0);
      expect(response.model).toBeDefined();
      expect(response.usage).toBeDefined();
      expect(response.usage!.totalTokens).toBeGreaterThan(0);

      console.log("âœ… Basic chat successful");
      console.log(`ðŸ“ Response: "${response.content}"`);
      console.log(`ðŸ¤– Model: ${response.model}`);
      console.log(`ðŸ“Š Tokens: ${response.usage!.totalTokens}`);
    });

    it("should handle multiple messages in conversation", async () => {
      const response = await llmClient.chat(
        [
          { role: "user", content: "Remember the number 42" },
          { role: "assistant", content: "I will remember the number 42." },
          { role: "user", content: "What number did I ask you to remember?" },
        ],
        { temperature: 0.1, maxTokens: 50 },
      );

      expect(response.content).toContain("42");
      console.log("âœ… Conversation context maintained");
    });

    it("should respect temperature and token limits", async () => {
      const response = await llmClient.chat(
        [{ role: "user", content: "Tell me a short joke" }],
        { temperature: 0.9, maxTokens: 200 },
      );

      expect(response.content.length).toBeGreaterThan(0);
      expect(response.usage!.completionTokens).toBeLessThanOrEqual(200);
      console.log("âœ… Temperature and token limits respected");
    });
  });

  describe("Expense Text Parsing", () => {
    const accounts = (global as any).testConfig.accounts;
    const tags = (global as any).testConfig.tags;

    test.each(TEST_EXPENSES)(
      'should parse expense: "%s"',
      async (expenseText) => {
        console.log(`\nðŸ“„ Testing: "${expenseText}"`);

        const result = await parseExpenseText(
          llmClient,
          expenseText,
          accounts,
          tags,
          "test-expense",
        );

        expect(result).toBeDefined();
        expect(result.toon).toBeDefined();

        // Confidence is currently always undefined - this should be updated when confidence calculation is implemented
        expect(result.confidence).toBe(undefined);

        // Try to parse TOON as JSON - handle cases where it might not be valid JSON yet
        let parsedTOON;
        try {
          parsedTOON = JSON.parse(result.toon);
          console.log(`ðŸ’° Parsed: ${JSON.stringify(parsedTOON, null, 2)}`);

          // Basic structure validation
          if (parsedTOON.action) {
            expect(parsedTOON.action).toBeDefined();
            expect(parsedTOON.params).toBeDefined();
          }
        } catch (e) {
          console.log(
            `âš ï¸ TOON not valid JSON (this is expected if parsing failed): ${result.toon}`,
          );
          // It's okay if parsing fails - just verify TOON content exists
          expect(result.toon).toBeDefined();
          expect(result.toon.length).toBeGreaterThan(0);
        }

        console.log(`ðŸŽ¯ Confidence: ${result.confidence}`);
      },
    );

    it('should parse amounts with "k" notation correctly', async () => {
      const result = await parseExpenseText(
        llmClient,
        "Lunch 120k at McDonalds",
        accounts,
        tags,
        "test-amount",
      );

      const parsedTOON = JSON.parse(result.toon);

      if (parsedTOON.params?.vnd_amount) {
        expect(parsedTOON.params.vnd_amount).toBe(120000);
        console.log('âœ… Amount parsing with "k" notation works');
      }
    });

    it("should identify merchants correctly", async () => {
      const result = await parseExpenseText(
        llmClient,
        "Coffee at Highlands Coffee",
        accounts,
        tags,
        "test-merchant",
      );

      const parsedTOON = JSON.parse(result.toon);

      if (parsedTOON.params?.counterparty) {
        expect(parsedTOON.params.counterparty.toLowerCase()).toContain(
          "highlands",
        );
        console.log("âœ… Merchant identification works");
      }
    });

    it("should use provided accounts and tags", async () => {
      const result = await parseExpenseText(
        llmClient,
        "Lunch at restaurant paid with cash",
        accounts,
        tags,
        "test-context",
      );

      const parsedTOON = JSON.parse(result.toon);

      // Should use cash account if mentioned
      if (parsedTOON.params?.account) {
        const accountNames = accounts.map((a: any) => a.name.toLowerCase());
        expect(accountNames).toContain(parsedTOON.params.account.toLowerCase());
        console.log("âœ… Uses provided account context");
      }
    });
  });

  describe("Complex Reasoning", () => {
    it("should handle complex expense descriptions", async () => {
      const complexPrompt = `
Analyze this expense: "Family dinner at Pizza Company for 4 people yesterday, total bill 850k, paid with Techcombank card, 10% service charge included"

Extract and respond with JSON containing:
- amount_vnd: total amount in numbers
- people_count: number of people
- restaurant: restaurant name
- payment_method: how it was paid
- service_charge: service charge percentage
`;

      const response = await llmClient.chat(
        [{ role: "user", content: complexPrompt }],
        { temperature: 0.1, maxTokens: 500 },
      );

      expect(response.content).toBeDefined();
      expect(response.content.length).toBeGreaterThan(0);

      // Try to parse JSON response
      try {
        const parsed = JSON.parse(response.content);
        expect(parsed.amount_vnd).toBeDefined();
        expect(parsed.people_count).toBeDefined();
        expect(parsed.restaurant).toBeDefined();
        expect(parsed.payment_method).toBeDefined();

        console.log("âœ… Complex reasoning successful");
        console.log(`ðŸ§  Extracted: ${JSON.stringify(parsed, null, 2)}`);
      } catch (e) {
        // If not valid JSON, that's still acceptable for some models
        console.log(
          "âš ï¸ Response not in JSON format, but content was generated",
        );
        expect(response.content).toContain("850");
      }
    });

    it("should handle Vietnamese currency and context", async () => {
      const response = await llmClient.chat(
        [
          {
            role: "user",
            content:
              'Parse this Vietnamese expense: "CÃ  phÃª 50k táº¡i Highlands Coffee sÃ¡ng nay" and respond with JSON',
          },
        ],
        { temperature: 0.1, maxTokens: 200 },
      );

      expect(response.content).toBeDefined();
      expect(response.content.length).toBeGreaterThan(0);
      expect(response.content).toMatch(/50|50000/);

      console.log("âœ… Vietnamese language handling works");
    });
  });

  describe("Error Handling", () => {
    it("should handle authentication errors gracefully", async () => {
      // Use a completely invalid base URL to force a network error
      const invalidClient = new LLMClient({
        provider: "anthropic",
        apiKey: "invalid-token-key",
        baseURL: "https://invalid-url-that-will-fail.com/api",
        timeout: 5000,
      });

      await expect(
        invalidClient.chat([{ role: "user", content: "This should fail" }]),
      ).rejects.toThrow();

      console.log("âœ… Authentication errors handled gracefully");
    });

    it("should handle timeout errors", async () => {
      const timeoutClient = new LLMClient({
        provider: "anthropic",
        apiKey: (global as any).testConfig.anthropic.apiKey,
        baseURL: (global as any).testConfig.anthropic.baseURL,
        timeout: 1, // 1ms timeout
      });

      await expect(
        timeoutClient.chat([{ role: "user", content: "This should timeout" }]),
      ).rejects.toThrow();

      console.log("âœ… Timeout errors handled gracefully");
    });
  });

  describe("Model Information", () => {
    it("should return correct model information", async () => {
      const response = await llmClient.chat([
        { role: "user", content: "Hello" },
      ]);

      expect(response.model).toBeDefined();
      expect(typeof response.model).toBe("string");

      // Should use the model from the proxy (likely glm-4.5-air)
      console.log(`ðŸ¤– Using model: ${response.model}`);
    });

    it("should provide usage statistics", async () => {
      const response = await llmClient.chat([
        { role: "user", content: "Count to 10" },
      ]);

      expect(response.usage).toBeDefined();
      expect(response.usage!.promptTokens).toBeGreaterThan(0);
      expect(response.usage!.completionTokens).toBeGreaterThan(0);
      expect(response.usage!.totalTokens).toBeGreaterThan(0);

      expect(response.usage!.totalTokens).toBe(
        response.usage!.promptTokens + response.usage!.completionTokens,
      );

      console.log("âœ… Usage statistics accurate");
    });
  });

  afterAll(() => {
    console.log("\nðŸŽ‰ All Anthropic LLM integration tests completed!");
  });
});
